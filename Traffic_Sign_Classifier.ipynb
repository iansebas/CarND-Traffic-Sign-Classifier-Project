{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = \"train.p\"\n",
    "validation_file= \"valid.p\"\n",
    "testing_file = \"test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "print(\"Finished Loading Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES** \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide a Basic Summary of the Data Set Using Python, Numpy and/or Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "n_validation = X_valid.shape[0]\n",
    "\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "image_shape = X_train[0].shape\n",
    "\n",
    "n_classes = np.unique(y_train).shape[0]\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include an exploratory visualization of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "\n",
    "# Distributions\n",
    "binwidth = 1\n",
    "plt.hist(y_train, bins=np.arange(min(y_train), max(y_train) + binwidth, binwidth))\n",
    "plt.title(\"Training Data Distribution by Class\")\n",
    "plt.show()\n",
    "plt.hist(y_valid, bins=np.arange(min(y_valid), max(y_valid) + binwidth, binwidth))\n",
    "plt.title(\"Validation Data Distribution by Class\")\n",
    "plt.show()\n",
    "plt.hist(y_test, bins=np.arange(min(y_test), max(y_test) + binwidth, binwidth))\n",
    "plt.title(\"Testing Data Distribution by Class\")\n",
    "plt.show()\n",
    "\n",
    "# Example image\n",
    "\n",
    "index = random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image)\n",
    "print(y_train[index])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data Set (normalization, grayscale, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimally, the image data should be normalized so that the data has mean zero and equal variance. For image data, `(pixel - 128)/ 128` is a quick way to approximately normalize the data and can be used in this project. \n",
    "\n",
    "Other pre-processing steps are optional. You can try different techniques to see if it improves performance. \n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "\n",
    "# Normalize Data\n",
    "X_train = (X_train - 128)/128\n",
    "X_valid = (X_valid - 128)/128\n",
    "X_test = (X_test - 128)/128\n",
    "\n",
    "# Example image\n",
    "index = random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image)\n",
    "print(y_train[index])\n",
    "\n",
    "# Balance Data\n",
    "import gc\n",
    "def balance_data(train_x, train_y):\n",
    "    \"\"\"Makes ALL classes have the same distribution\"\"\"\n",
    "\n",
    "    print('Original Dataset : {}'.format(train_x.shape[0]))\n",
    "\n",
    "    max_count_index = np.bincount(train_y).argmax()\n",
    "    max_count = np.where(train_y == max_count_index)[0].shape[0]\n",
    "    \n",
    "    print(\"Largest class has {} elements\".format(max_count))\n",
    "\n",
    "    xx = np.copy(train_x)\n",
    "    yy = np.copy(train_y)\n",
    "\n",
    "    #Over-Sampling\n",
    "    for j in range(n_classes):\n",
    "        mask_n = np.where(train_y == j)[0]\n",
    "        num_to_oversample = max_count -  np.where(train_y == j)[0].shape[0]\n",
    "        print(\"Filling class {} with {} elements\".format(j,num_to_oversample))\n",
    "        x_n = train_x[mask_n]\n",
    "        y_n = train_y[mask_n]\n",
    "        b_mask = np.random.RandomState().choice(x_n.shape[0], num_to_oversample)\n",
    "        x_k = x_n[[b_mask]]\n",
    "        y_k = y_n[[b_mask]]\n",
    "        xx = np.concatenate((xx,x_k),axis=0)\n",
    "        yy = np.concatenate((yy,y_k),axis=0)\n",
    "        del x_n, y_n, x_k, y_k, mask_n, b_mask\n",
    "        gc.collect()\n",
    "        print(\"Class {} has now {} elements\".format(j,np.where(yy == j)[0].shape[0]))\n",
    "\n",
    "    print('Augmented Dataset : {}'.format(xx.shape[0]))\n",
    "    \n",
    "    return xx, yy\n",
    "\n",
    "# X_train, y_train = balance_data(X_train, y_train)\n",
    "# Decided againts balancing the data\n",
    "\n",
    "print(\"Finished Preprocessing Data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def LeNet(x):    \n",
    "    conv1_W = tf.get_variable(\"conv1_W\", shape = [5, 5, 3, 6], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    conv1_b = tf.get_variable(\"conv1_b\", shape = [6], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2_W = tf.get_variable(\"conv2_W\", shape = [5, 5, 6, 16], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    conv2_b = tf.get_variable(\"conv2_b\", shape = [16], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n",
    "    fc0   =  tf.contrib.layers.flatten(conv2)\n",
    "    \n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_W = tf.get_variable(\"fc1_W\", shape = [400, 120], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    fc1_b = tf.get_variable(\"fc1_b\", shape = [120], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_W = tf.get_variable(\"fc2_W\", shape = [120, 84], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    fc2_b = tf.get_variable(\"fc2_b\", shape = [84], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "\n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = n_classes.\n",
    "    fc3_W = tf.get_variable(\"fc3_W\", shape = [84, n_classes], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    fc3_b = tf.get_variable(\"fc3_b\", shape = [n_classes], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def dnn(x):\n",
    "    \n",
    "    y = LeNet(x)\n",
    "    \n",
    "    return y\n",
    "\n",
    "print(\"Finished Defining DNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper Definitions\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, n_classes)\n",
    "\n",
    "logits = dnn(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print(\"Finished Loading Graph\")\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "# Train & Validate\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        saver.save(sess, 'model')\n",
    "        \n",
    "    saver.save(sess, 'model')\n",
    "    print(\"Model saved\")\n",
    "    \n",
    "    print(\"Testing...\")\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n",
    "    sess.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "To give yourself more insight into how your model is working, download at least five pictures of German traffic signs from the web and use your model to predict the traffic sign type.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Output the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.misc import imresize\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "\n",
    "n_images = 5\n",
    "\n",
    "X_data = np.random.rand(n_images, 32,32,3)\n",
    "\n",
    "y_data = np.random.rand(n_images)\n",
    "\n",
    "info = pd.read_csv(\"test.csv\", sep=';',header=0, names=['Filename', 'Width', 'Height','Roi.X1','Roi.Y1','Roi.X2','Roi.Y2','ClassId'])\n",
    "mappings = pd.read_csv(\"signnames.csv\", sep=',',header=0, names=['ClassId', 'SignName'])\n",
    "\n",
    "for  i in range(n_images):\n",
    "    file_name = \"{:05d}.ppm\".format(randint(0,12000))\n",
    "    # X\n",
    "    xx =  mpimg.imread(\"images/{}\".format(file_name))\n",
    "    xx = imresize(xx, (32, 32))\n",
    "    X_data[i] = xx\n",
    "    # Y\n",
    "    yy = info.loc[info['Filename'] == file_name].iloc[0]['ClassId']\n",
    "    y_data[i] = yy\n",
    "    # Visualize\n",
    "    sign_name = mappings.loc[mappings['ClassId'] == yy].iloc[0]['SignName']\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.title(sign_name)\n",
    "    plt.imshow(xx)\n",
    "    \n",
    "X_data = (X_data - 128)/128\n",
    "\n",
    "print(\"Finished loading images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the Sign Type for Each Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_vec = tf.nn.softmax(logits)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model')\n",
    "    for  i in range(X_data.shape[0]):\n",
    "        probs = sess.run(prob_vec, feed_dict={x: np.expand_dims(X_data[i],axis=0)})\n",
    "        prediction = np.argmax(probs)\n",
    "        sign_name = mappings.loc[mappings['ClassId'] == prediction].iloc[0]['SignName']\n",
    "        print(\"Sign number {} is predicted to be a {}\".format(i,sign_name))\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model')\n",
    "    accuracy = evaluate(X_data, y_data)\n",
    "    sess.close()\n",
    "print(\"Accuracy = {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Top 5 Softmax Probabilities For Each Image Found on the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model')\n",
    "    all_values, all_indices = sess.run(tf.nn.top_k(prob_vec, k=top_k), feed_dict={x: X_data})\n",
    "    i = 0\n",
    "    sess.close()\n",
    "\n",
    "for values, indices in zip(all_values,all_indices):\n",
    "    n = 1\n",
    "    print(\"\\n\")\n",
    "    print(\"Sign #{}\".format(i))\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(X_data[i]*128+128)\n",
    "    plt.show()\n",
    "    sign_names = []\n",
    "    for index in indices:\n",
    "        sign_name = mappings.loc[mappings['ClassId'] == index].iloc[0]['SignName']\n",
    "        sign_names.append(sign_name)\n",
    "        print(\"The top {} probability for Sign #{} is to be a {}\".format(n,i,sign_name))\n",
    "        n += 1\n",
    "    plt.barh(np.arange(len(sign_names)), values, align='center', alpha=0.5)\n",
    "    plt.yticks(np.arange(len(sign_names)),sign_names)\n",
    "    plt.xlabel('Probabilities')\n",
    "    plt.title('Sign #{}'.format(i))\n",
    "    plt.show()\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
