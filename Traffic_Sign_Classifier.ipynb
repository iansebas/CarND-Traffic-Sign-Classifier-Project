{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = \"train.p\"\n",
    "validation_file= \"valid.p\"\n",
    "testing_file = \"test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "print(\"Finished Loading Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide a Basic Summary of the Data Set Using Python, Numpy and/or Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "n_validation = X_valid.shape[0]\n",
    "\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "image_shape = X_train[0].shape\n",
    "\n",
    "n_classes = np.unique(y_train).shape[0]\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include an exploratory visualization of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "\n",
    "# Distributions\n",
    "binwidth = 1\n",
    "plt.hist(y_train, bins=np.arange(min(y_train), max(y_train) + binwidth, binwidth))\n",
    "plt.title(\"Training Data Distribution by Class\")\n",
    "plt.show()\n",
    "plt.hist(y_valid, bins=np.arange(min(y_valid), max(y_valid) + binwidth, binwidth))\n",
    "plt.title(\"Validation Data Distribution by Class\")\n",
    "plt.show()\n",
    "plt.hist(y_test, bins=np.arange(min(y_test), max(y_test) + binwidth, binwidth))\n",
    "plt.title(\"Testing Data Distribution by Class\")\n",
    "plt.show()\n",
    "\n",
    "# Example image\n",
    "\n",
    "index = random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image)\n",
    "print(y_train[index])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data Set (normalization, grayscale, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "\n",
    "# Normalize Data\n",
    "X_train = (X_train - 128.0)/128.0\n",
    "X_valid = (X_valid - 128.0)/128.0\n",
    "X_test = (X_test - 128.0)/128.0\n",
    "\n",
    "# Example image\n",
    "index = random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image)\n",
    "print(y_train[index])\n",
    "\n",
    "# Balance Data\n",
    "import gc\n",
    "def balance_data(train_x, train_y):\n",
    "    \"\"\"Makes ALL classes have the same distribution\"\"\"\n",
    "\n",
    "    print('Original Dataset : {}'.format(train_x.shape[0]))\n",
    "\n",
    "    max_count_index = np.bincount(train_y).argmax()\n",
    "    max_count = np.where(train_y == max_count_index)[0].shape[0]\n",
    "    \n",
    "    print(\"Largest class has {} elements\".format(max_count))\n",
    "\n",
    "    xx = np.copy(train_x)\n",
    "    yy = np.copy(train_y)\n",
    "\n",
    "    #Over-Sampling\n",
    "    for j in range(n_classes):\n",
    "        mask_n = np.where(train_y == j)[0]\n",
    "        num_to_oversample = max_count -  np.where(train_y == j)[0].shape[0]\n",
    "        print(\"Filling class {} with {} elements\".format(j,num_to_oversample))\n",
    "        x_n = train_x[mask_n]\n",
    "        y_n = train_y[mask_n]\n",
    "        b_mask = np.random.RandomState().choice(x_n.shape[0], num_to_oversample)\n",
    "        x_k = x_n[[b_mask]]\n",
    "        y_k = y_n[[b_mask]]\n",
    "        xx = np.concatenate((xx,x_k),axis=0)\n",
    "        yy = np.concatenate((yy,y_k),axis=0)\n",
    "        del x_n, y_n, x_k, y_k, mask_n, b_mask\n",
    "        gc.collect()\n",
    "        print(\"Class {} has now {} elements\".format(j,np.where(yy == j)[0].shape[0]))\n",
    "\n",
    "    print('Augmented Dataset : {}'.format(xx.shape[0]))\n",
    "    \n",
    "    return xx, yy\n",
    "\n",
    "# X_train, y_train = balance_data(X_train, y_train)\n",
    "# Decided againts balancing the data\n",
    "\n",
    "print(\"Finished Preprocessing Data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def LeNet(x):    \n",
    "    conv1_W = tf.get_variable(\"conv1_W\", shape = [5, 5, 3, 6], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    conv1_b = tf.get_variable(\"conv1_b\", shape = [6], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2_W = tf.get_variable(\"conv2_W\", shape = [5, 5, 6, 16], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    conv2_b = tf.get_variable(\"conv2_b\", shape = [16], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n",
    "    fc0   =  tf.contrib.layers.flatten(conv2)\n",
    "    \n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_W = tf.get_variable(\"fc1_W\", shape = [400, 120], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    fc1_b = tf.get_variable(\"fc1_b\", shape = [120], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_W = tf.get_variable(\"fc2_W\", shape = [120, 84], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    fc2_b = tf.get_variable(\"fc2_b\", shape = [84], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "\n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = n_classes.\n",
    "    fc3_W = tf.get_variable(\"fc3_W\", shape = [84, n_classes], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    fc3_b = tf.get_variable(\"fc3_b\", shape = [n_classes], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def dnn(x):\n",
    "    \n",
    "    y = LeNet(x)\n",
    "    \n",
    "    return y\n",
    "\n",
    "print(\"Finished Defining DNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper Definitions\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, n_classes)\n",
    "\n",
    "logits = dnn(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print(\"Finished Loading Graph\")\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "# Train & Validate\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        saver.save(sess, 'model')\n",
    "        \n",
    "    saver.save(sess, 'model')\n",
    "    print(\"Model saved\")\n",
    "    \n",
    "    print(\"Testing...\")\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n",
    "    sess.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "To give yourself more insight into how your model is working, download at least five pictures of German traffic signs from the web and use your model to predict the traffic sign type.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Output the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.misc import imresize\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "\n",
    "n_images = 5\n",
    "\n",
    "X_data = np.random.rand(n_images, 32,32,3)\n",
    "\n",
    "y_data = np.random.rand(n_images)\n",
    "\n",
    "info = pd.read_csv(\"test.csv\", sep=';',header=0, names=['Filename', 'Width', 'Height','Roi.X1','Roi.Y1','Roi.X2','Roi.Y2','ClassId'])\n",
    "mappings = pd.read_csv(\"signnames.csv\", sep=',',header=0, names=['ClassId', 'SignName'])\n",
    "\n",
    "for  i in range(n_images):\n",
    "    file_name = \"{:05d}.ppm\".format(randint(0,12000))\n",
    "    # X\n",
    "    xx =  mpimg.imread(\"images/{}\".format(file_name))\n",
    "    xx = imresize(xx, (32, 32))\n",
    "    X_data[i] = xx\n",
    "    # Y\n",
    "    yy = info.loc[info['Filename'] == file_name].iloc[0]['ClassId']\n",
    "    y_data[i] = yy\n",
    "    # Visualize\n",
    "    sign_name = mappings.loc[mappings['ClassId'] == yy].iloc[0]['SignName']\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.title(sign_name)\n",
    "    plt.imshow(xx)\n",
    "    \n",
    "X_data = (X_data - 128.0)/128.0\n",
    "\n",
    "print(\"Finished loading images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the Sign Type for Each Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_vec = tf.nn.softmax(logits)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model')\n",
    "    for  i in range(X_data.shape[0]):\n",
    "        probs = sess.run(prob_vec, feed_dict={x: np.expand_dims(X_data[i],axis=0)})\n",
    "        prediction = np.argmax(probs)\n",
    "        sign_name = mappings.loc[mappings['ClassId'] == prediction].iloc[0]['SignName']\n",
    "        print(\"Sign number {} is predicted to be a {}\".format(i,sign_name))\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model')\n",
    "    accuracy = evaluate(X_data, y_data)\n",
    "    sess.close()\n",
    "print(\"Accuracy = {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Top 5 Softmax Probabilities For Each Image Found on the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model')\n",
    "    all_values, all_indices = sess.run(tf.nn.top_k(prob_vec, k=top_k), feed_dict={x: X_data})\n",
    "    i = 0\n",
    "    sess.close()\n",
    "\n",
    "for values, indices in zip(all_values,all_indices):\n",
    "    n = 1\n",
    "    print(\"\\n\")\n",
    "    print(\"Sign #{}\".format(i))\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(X_data[i]*128.0+128.0)\n",
    "    plt.show()\n",
    "    sign_names = []\n",
    "    for index in indices:\n",
    "        sign_name = mappings.loc[mappings['ClassId'] == index].iloc[0]['SignName']\n",
    "        sign_names.append(sign_name)\n",
    "        print(\"The top {} probability for Sign #{} is to be a {}\".format(n,i,sign_name))\n",
    "        n += 1\n",
    "    plt.barh(np.arange(len(sign_names)), values, align='center', alpha=0.5)\n",
    "    plt.yticks(np.arange(len(sign_names)),sign_names)\n",
    "    plt.xlabel('Probabilities')\n",
    "    plt.title('Sign #{}'.format(i))\n",
    "    plt.show()\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
